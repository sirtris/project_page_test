<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>DEEP-EM TOOLBOX</title>
<!-- TODO: add a cool favicon -->
<!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">DEEP-EM TOOLBOX: Deep Learning Toolbox for Electron Microscopy Researchers</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://viscom.uni-ulm.de/members/hannah-kniesel/" target="_blank">Hannah Kniesel</a>,</span>
                <span class="author-block">
                  <a href="https://viscom.uni-ulm.de/members/tristan-payer/" target="_blank">Tristan Payer</a>,</span>
                  <span class="author-block">
                    <a href="https://viscom.uni-ulm.de/members/poonam/" target="_blank">Poonam Poonam</a>,
                  </span>
                <span class="author-block">
                    <a href="https://viscom.uni-ulm.de/members/timo-ropinski/" target="_blank">Timo Ropinski</a>
                </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Visual Computing Group at Ulm University<br>Conferance name and year</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
    <img src="static/images/tasks.png" alt="Schematic showing 3 differnt types of task applicable for deep learning. (image to values, image to image & 2D to 3D)" />
      <h2 class="subtitle has-text-centered">Tasks in the Area of EM data analysis can be categorized by the requirements of the DL
    method into Image to Value(s), Image to Image and 2D to 3D.</h2>
    </div>
  </div>
</section>
<!-- End teaser image -->


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>This paper endeavors to narrow the gap between deep learning experts and electron microscopy (EM) researchers, recognizing the immense potential of deep learning in advancing the analysis of EM micrographs. With its proven efficacy in computer vision tasks, deep learning stands poised to revolutionize EM image analysis through supported, automated, and standardized methodologies. From the vantage point of deep learning experts, this work aims to inspire EM researchers to cultivate their own computational tools, leveraging their domain expertise to augment and refine analytical solutions.

We introduce a simple deep learning (DL) workflow to aid researchers in navigating the task of developing DL solutions in the context of EM. Additionally, we provide a small subset of example solutions for standard tasks such as detection, segmentation, and reconstruction, and we encourage further submissions from the research community. By fostering a collaborative ecosystem wherein novel developments and insights can be shared, refined, and disseminated, we endeavor to democratize access to deep learning techniques. This concerted effort aims to empower EM researchers to harness the full potential of this transformative technology in their pursuit of scientific discovery.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!--Intro to Deep Learning -->
<section class="section hero">
    <div class="container is-max-desktop content">
        <h2 class="title is-3">Intro to Deep Learning</h2>
        <div class="content has-text-justified">
          <p>A (short) introduction about Deep Learning will be given here.</p>
          <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna
              aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.
              Duis
              aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint
              occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>
        </div>
    </div>
</section>
<!--End intro to Deep Learning  -->



<!--Workflow -->
<section class="section hero is-light">
    <div class="container is-max-desktop content">
        <h2 class="title is-3">Workflow</h2>
        <div class="hero-body">
            <img src="static/images/workflow.png"
                alt="Schematic showing an example of a workflow for deep learning." />
            <h6 class="subtitle has-text-centered">Example workflow of a Deep Learning application.</h6>
        </div>
        <div class="content has-text-justified">
            <p>We introduce a simple Workflow of Deep Learning, which can be followed for the successful development of Deep
            Learning methods for supported EM analysis. In our workflow we propose 3 clusters:</p>
            <ul>
                <li>Task and model definition (orange).</li>
                <li>Data collection and preparation (green).</li>
                <li>Model development (red).</li>
            </ul>
            <h3>Task and model definition</h3>
            <p>The task definition influences the model choice later on and goes hand in hand with the data collection step.
               The deep learning workflow can be differentiated into either a task-driven pipeline or a data-driven pipeline.
               In the task driven approch the task that should be solved is defined first and then data is collected accordingly.
               A benefit of this is that it directly aims to solve a part of the EM research pipeline. A downside of this is that
               data aquisition requires additional time and effort and not every task of the EM research pipeline might easily be
                solved using deep learning.<br>
                The data driven approach on the other side aims to leverage data that is already available. An advantage is that it
                can create a stong foundation for future tasks. On the downside it might not be immeadeately clear how the available data
                might help to solve a task from the current EM research pipeline.
            </p>
            <p>
                A structured definition of the task involves several
                key considerations. First, we need to clearly define the input data and the expected output. This
                clarity ensures that the task is well understood and that the goals are achievable. It is also essential to
                assess whether the output can be logically derived from the input data, ensuring there is a meaningful
                correlation between them.<br>
                In general, it is possible to design architectures yourself from scratch. However, this is not recom-
                mended. First, it requires a lot of compute resources and time to design a well fitted architecture.
                Second, there are many well designed architectures, which are frequently used, hence sufficiently tested,
                as well as there are pretrained models freely available.<br>
                Furthermore, when selecting a model backbone one should consider that the type matches the data characteristics and
                balance the number of trainable parameters with the available training data. Having a large number of trainable
                parameters with a small amount of data will likely lead to overfitting.
            </p>

            <h3>Data collection and preparation</h3>
            <p>
                In deep learning, it is commonly believed that more data leads to better model performance. However,
                this is not always true; High-quality data is essential to derive correlations between model inputs and
                outputs, providing a strong learning signal. Achieving optimal model performance requires a balance between data quality, variance, robustness,
                and dataset size.
            </p>
            <p>
                For data acquisition in electron microscopy (EM), there are three primary sources: reusing existing
                data, acquiring new data, and generating synthetic data, each with distinct benefits and downsides.<br>
                During data collection, it is crucial to investigate potential correlations between model inputs
                and outputs, as a deep learning model can only be trained successfully if such correlations exist.
                Visualization techniques are vital in this process. Visualizing raw image grids helps assess image
                quality, detect artifacts, and understand data variation.
            </p>
            <p>
                High-quality annotations are similarly crucial for the performance of deep learning models as large
                dataset sizes. <br>
                Annotation types are strongly correlated with deep learning tasks and can be
                differentiated into several categories: classification labels, regression labels, keypoints, bounding boxes
                and segmentation masks. These can be further categorized based on their complexity
                into image-level, object/instance-level, and pixel-level annotations.
            </p>
            <p>
                Effective data preprocessing plays a vital role in preparing high-quality datasets for deep learning
                models. While deep learning models thrive on large dataset sizes, the quality of the data is equally
                important and can be ensured by detailed data preprocessing.
            </p>
            <h3> Model development</h3>
            <p>
                During model training the parameters of the selected architecture are adapted in such way, that the
                model is able to approximate input-output relations within the data as precise as possible. However,
                it also involves correct initialization of the model parameters, suitable tuning of hyperparameters, and
                suitable monitoring of the training process to identify possible pitfalls.
            </p>
                Training a machine learning model involves initializing its parameters, and proper initialization is crucial for efficient training. Basic training from scratch, where parameters are randomly initialized, is viable only with large, diverse datasets. In electron microscopy (EM), where datasets are typically small and specific, training from scratch often leads to poor generalization. Using pretrained weights from models trained on similar tasks but larger datasets is more effective. Pretrained models, even those with a domain gap like ImageNet-pretrained models, can improve training speed and performance.<br>

Hyperparameter tuning is essential for optimizing model performance. This involves experimenting with various settings for parameters like learning rate, batch size, and optimizer. Techniques such as grid search, random search, and Bayesian optimization are commonly used. Tools like Weights & Biases can streamline hyperparameter tuning.<br>

Effective logging of the training process is critical. Tools like TensorBoard and MLflow help monitor metrics and losses for training, validation, and test datasets. Logging can identify issues like overfitting, where the model performs well on training data but poorly on validation data. Strategies to mitigate overfitting include early stopping, data augmentation, and regularization techniques like dropout and batch normalization.<br>

Visualizations of model inputs, outputs, and learned features provide insights into model performance and behavior. Techniques like Grad-CAM, t-SNE, and PCA help understand model decisions and reveal biases. Saving model weights and optimizer states regularly allows resuming training if interrupted and facilitates using intermediate models. Logging gradients can identify issues like exploding or vanishing gradients, which can be addressed by adjusting the learning rate, gradient clipping, or adding skip connections.<br>
            </p>
        </div>
    </div>
</section>
<!--End workflow  -->



<!--Links -->
<section class="section hero">
    <div class="container is-max-desktop content">
        <h2 class="title is-3">Example notebooks</h2>
        <div class="content has-text-justified">
            <p>Here will be a section to some example notebooks that you can run.</p>
            <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et
                dolore magna
                aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo
                consequat. Duis
                aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.
                Excepteur sint
                occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>
        </div>
    </div>
</section>
<!--End Links  -->



<!--BibTex citation -->
  <section class="section is-light" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->

<!-- Footer  -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>
<!-- End footer -->

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
